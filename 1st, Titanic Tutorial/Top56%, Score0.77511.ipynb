{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_Titanic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMQim_8KC-aS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYWDjPcfDwNz"
      },
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/#Kaggle/titanic/test.csv\")\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/#Kaggle/titanic/train.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Z1-FNdEBIL",
        "outputId": "5b699591-1b6e-4779-ea9a-8612d6ce31fd"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBIxan3CD4nZ"
      },
      "source": [
        "column_label = train_df.columns"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oICiiNOFkMS"
      },
      "source": [
        "#### 전체 column \n",
        "승객 아이디, 생존여부, 승선등급, 이름, 성, 나이, 형제자매/배우자, 부모/자식 수, 티켓 ID?, 선실번호, 승선위치 \n",
        "\n",
        "#### 생각\n",
        "1. 승객 아이디, 이름, 티켓 아이디는 너무 데이터가 난해해서 필요 없을 거 같음\n",
        "2. Embarked가 승선위치인데, 3가지의 string으로 구분되어 있음, 이를 one-hot 으로 바꿔서 학습 시켜보면 어떨꽈 \n",
        "3. Nan 값에 대해서, 필드가 다양하니 단순 평균 말고 다른 columns(feature)에 대해서 최대한 일치하는 것으로 평균을 잡아서 \n",
        "4. Cabin이 선실을 예약한 사람들의 기록이 있는 사람이라고 생각해서 값이 있으면 1, 없으면 0 으로 하려고 했는데 값이 있는 사람들이 204/891 말이 안되는듯 \n",
        "\n",
        "\n",
        "##### 삭제 \n",
        "-> P_Id, Name, Ticket, Cabin \n",
        "\n",
        "##### 변형 \n",
        "-> Embarked \n",
        "\n",
        "##### Data Imputation \n",
        "-> 필드가 최대한 일치하는 친구들과 함께 평균내서 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcTU5_naETjs",
        "outputId": "f64510bb-abd2-47f4-eeee-c8cada1529f8"
      },
      "source": [
        "feature_label = list(train_df.columns)\n",
        "feature_label.remove(\"PassengerId\")\n",
        "feature_label.remove(\"Name\")\n",
        "feature_label.remove(\"Cabin\")\n",
        "feature_label.remove(\"Ticket\")\n",
        "feature_label"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILOeFVslOI_R",
        "outputId": "bf1bc0ee-809a-4764-8ee2-78c688e0b335"
      },
      "source": [
        "selected_df = train_df[feature_label]\n",
        "selected_df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 8 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Survived  891 non-null    int64  \n",
            " 1   Pclass    891 non-null    int64  \n",
            " 2   Sex       891 non-null    object \n",
            " 3   Age       714 non-null    float64\n",
            " 4   SibSp     891 non-null    int64  \n",
            " 5   Parch     891 non-null    int64  \n",
            " 6   Fare      891 non-null    float64\n",
            " 7   Embarked  889 non-null    object \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 55.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBs57QrhACa"
      },
      "source": [
        "# 쉽게 쓰이는거 모음, 리턴은 3개를 다 리턴해줌 \n",
        "def dataProcessing(srcDataFrame, resetIndex = True, printInfo=False):\n",
        "  returnList = []\n",
        "\n",
        "  returnList.append(srcDataFrame.fillna(srcDataFrame.mean())) # selected_df.mean() -> 컬럼 별로 평균 값이 나옴 \n",
        "  returnList.append(srcDataFrame.fillna(0))\n",
        "  returnList.append(srcDataFrame.dropna())\n",
        "\n",
        "  if resetIndex :\n",
        "    returnList[-1].reset_index(drop=True)\n",
        "\n",
        "  if printInfo:\n",
        "    for unit in returnList:\n",
        "      print(unit)\n",
        "      unit.info()\n",
        "      print(unit.mean())\n",
        "      print(\"\\n\\n\")\n",
        "  \n",
        "  return returnList"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh3kScbnpK3k"
      },
      "source": [
        "processed_df = dataProcessing(selected_df)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL5cveaROy9A"
      },
      "source": [
        "def oneHotEncoding(srcDataFrame):\n",
        "  if type(srcDataFrame) is list :\n",
        "    \n",
        "    for idx, unit in enumerate(srcDataFrame):\n",
        "      srcDataFrame[idx] = pd.get_dummies(unit)\n",
        "\n",
        "  else :\n",
        "    srcDataFrame = pd.get_dummies(srcDataFrame)\n",
        "  \n",
        "  return srcDataFrame"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvxziMqQwAz"
      },
      "source": [
        "processed_df = oneHotEncoding(processed_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crTwe8CRSypz"
      },
      "source": [
        "# for validation train \n",
        "def trainTestSplit(srcDataframe, yLabel, ratio=0.8):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # *array -> lists, numpy, pandas dataframe \n",
        "  if type(srcDataframe) is list :\n",
        "    for idx, unit in enumerate(srcDataframe):\n",
        "      feature_index = list(unit.columns)\n",
        "      feature_index.remove(yLabel)\n",
        "      srcDataframe[idx] = train_test_split(unit[feature_index].to_numpy(),unit[yLabel].to_numpy(), train_size=ratio, random_state=411)\n",
        "  else:\n",
        "\n",
        "    feature_index = list(srcDataframe.columns)\n",
        "    feature_index.remove(yLabel)\n",
        "    srcDataframe = train_test_split(srcDataframe[feature_index].to_numpy(),srcDataframe[yLabel].to_numpy(), train_size=ratio, random_state=411)\n",
        "\n",
        "\n",
        "  return srcDataframe  \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQx2Lsx_fAwv"
      },
      "source": [
        "splitted = trainTestSplit(processed_df.copy(), 'Survived')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMcFfNFELdWw",
        "outputId": "9e7c439a-752a-412e-8118-1b90e86e9914"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
        "\n",
        "HBD = 411\n",
        "for i in range(len(splitted)):\n",
        "  train_X, test_X, train_y, test_y = splitted[i]\n",
        "\n",
        "  minmax = MinMaxScaler().fit(train_X)\n",
        "  Std = StandardScaler().fit(train_X)\n",
        "  maxAbs = MaxAbsScaler().fit(train_X)\n",
        "  robust = RobustScaler().fit(train_X)\n",
        "\n",
        "  minmaxTrain_X = minmax.transform(train_X)\n",
        "  StdTrain_X = Std.transform(train_X)\n",
        "  maxAbsTrain_X = maxAbs.transform(train_X)\n",
        "  robustTrain_X = robust.transform(train_X)\n",
        "\n",
        "  # minmax = MinMaxScaler().fit(test_X)\n",
        "  # Std = StandardScaler().fit(test_X)\n",
        "  # maxAbs = MaxAbsScaler().fit(test_X)\n",
        "  # robust = RobustScaler().fit(test_X)\n",
        "\n",
        "  minmaxtest_X = minmax.transform(test_X)\n",
        "  Stdtest_X = Std.transform(test_X)\n",
        "  maxAbstest_X = maxAbs.transform(test_X)\n",
        "  robusttest_X = robust.transform(test_X)\n",
        "\n",
        "\n",
        "  import numpy as np\n",
        "  # sklearn test \n",
        "  from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "  pac = PassiveAggressiveClassifier().fit(train_X, train_y)\n",
        "  print(\"PassiveAggressiveClassifier\", pac.score(test_X, test_y))\n",
        "\n",
        "  pac = PassiveAggressiveClassifier().fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", pac.score(minmaxtest_X, test_y))\n",
        "  pac = PassiveAggressiveClassifier().fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", pac.score(Stdtest_X, test_y))\n",
        "  pac = PassiveAggressiveClassifier().fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", pac.score(maxAbstest_X, test_y))\n",
        "  pac = PassiveAggressiveClassifier().fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", pac.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  from sklearn.linear_model import RidgeClassifier\n",
        "  ridge = RidgeClassifier().fit(train_X, train_y)\n",
        "  print(\"RidgeClassifier\",ridge.score(test_X, test_y))\n",
        "\n",
        "  ridge = RidgeClassifier().fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", ridge.score(minmaxtest_X, test_y))\n",
        "  ridge = RidgeClassifier().fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", ridge.score(Stdtest_X, test_y))\n",
        "  ridge = RidgeClassifier().fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", ridge.score(maxAbstest_X, test_y))\n",
        "  ridge = RidgeClassifier().fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", ridge.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  from sklearn.linear_model import RidgeClassifierCV\n",
        "  ridgeCv = RidgeClassifierCV().fit(train_X, train_y)\n",
        "  print(\"RidgeClassifierCV\",ridgeCv.score(test_X, test_y))\n",
        "\n",
        "  ridgeCv = RidgeClassifierCV().fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", ridgeCv.score(minmaxtest_X, test_y))\n",
        "  ridgeCv = RidgeClassifierCV().fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", ridgeCv.score(Stdtest_X, test_y))\n",
        "  ridgeCv = RidgeClassifierCV().fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", ridgeCv.score(maxAbstest_X, test_y))\n",
        "  ridgeCv = RidgeClassifierCV().fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", ridgeCv.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  from sklearn.linear_model import SGDClassifier\n",
        "  sgd = SGDClassifier().fit(train_X, train_y)\n",
        "  print(\"SGDClassifier\",sgd.score(test_X, test_y))\n",
        "\n",
        "  sgd = SGDClassifier().fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", sgd.score(minmaxtest_X, test_y))\n",
        "  sgd = SGDClassifier().fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", sgd.score(Stdtest_X, test_y))\n",
        "  sgd = SGDClassifier().fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", sgd.score(maxAbstest_X, test_y))\n",
        "  sgd = SGDClassifier().fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", sgd.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  from sklearn import svm\n",
        "  svc = svm.SVC(gamma='scale').fit(train_X, train_y)\n",
        "  print(\"SVC\",svc.score(test_X, test_y))\n",
        "\n",
        "  svc = svm.SVC(gamma='scale').fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", svc.score(minmaxtest_X, test_y))\n",
        "  svc = svm.SVC(gamma='scale').fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", svc.score(Stdtest_X, test_y))\n",
        "  svc = svm.SVC(gamma='scale').fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", svc.score(maxAbstest_X, test_y))\n",
        "  svc = svm.SVC(gamma='scale').fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", svc.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  from sklearn.neural_network import MLPClassifier\n",
        "  mlp = MLPClassifier(random_state=411).fit(train_X, train_y)\n",
        "  print(\"MLPClassifier\", mlp.score(test_X, test_y))\n",
        "\n",
        "  mlp = MLPClassifier(random_state=411).fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", mlp.score(minmaxtest_X, test_y))\n",
        "  mlp = MLPClassifier(random_state=411).fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", mlp.score(Stdtest_X, test_y))\n",
        "  mlp = MLPClassifier(random_state=411).fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", mlp.score(maxAbstest_X, test_y))\n",
        "  mlp = MLPClassifier(random_state=411).fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", mlp.score(robusttest_X, test_y))\n",
        "  print()\n",
        "\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(train_X, train_y)\n",
        "  print(\"MLPClassifier\",mlp.score(test_X, test_y))\n",
        "\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(minmaxTrain_X, train_y)\n",
        "  print(\"MinMaxScaler\", mlp.score(minmaxtest_X, test_y))\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(StdTrain_X, train_y)\n",
        "  print(\"StandardScaler\", mlp.score(Stdtest_X, test_y))\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(maxAbsTrain_X, train_y)\n",
        "  print(\"MaxAbsScaler\", mlp.score(maxAbstest_X, test_y))\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(robustTrain_X, train_y)\n",
        "  print(\"RobustScaler\", mlp.score(robusttest_X, test_y))\n",
        "\n",
        "  from xgboost import XGBClassifier\n",
        "\n",
        "  xgb = XGBClassifier(n_estimators =500, random_state=411)\n",
        "\n",
        "  eval_set = [(train_X, train_y), (test_X, test_y)] \n",
        "  eval_metric = [\"auc\", \"error\"]\n",
        "  xgb.fit(train_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
        "  print(\"XGboost :\", xgb.score(test_X, test_y))\n",
        "\n",
        "  xgb = XGBClassifier(n_estimators =500, random_state=411)\n",
        "  eval_set = [(minmaxTrain_X, train_y), (minmaxtest_X, test_y)] \n",
        "  xgb.fit(minmaxTrain_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
        "  print(\"XGboost :\", xgb.score(minmaxtest_X, test_y))\n",
        "\n",
        "  xgb = XGBClassifier(n_estimators =500, random_state=411)\n",
        "  eval_set = [(StdTrain_X, train_y), (Stdtest_X, test_y)] \n",
        "  xgb.fit(StdTrain_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
        "  print(\"XGboost :\", xgb.score(Stdtest_X, test_y))\n",
        "\n",
        "  xgb = XGBClassifier(n_estimators =500, random_state=411)\n",
        "  eval_set = [(maxAbsTrain_X, train_y), (maxAbstest_X, test_y)] \n",
        "  xgb.fit(maxAbsTrain_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
        "  print(\"XGboost :\", xgb.score(maxAbstest_X, test_y))\n",
        "\n",
        "  xgb = XGBClassifier(n_estimators =500, random_state=411)\n",
        "  eval_set = [(robustTrain_X, train_y), (robusttest_X, test_y)] \n",
        "  xgb.fit(robustTrain_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
        "  print(\"XGboost :\", xgb.score(robusttest_X, test_y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PassiveAggressiveClassifier 0.3687150837988827\n",
            "MinMaxScaler 0.776536312849162\n",
            "StandardScaler 0.7039106145251397\n",
            "MaxAbsScaler 0.7988826815642458\n",
            "RobustScaler 0.7597765363128491\n",
            "\n",
            "RidgeClassifier 0.7988826815642458\n",
            "MinMaxScaler 0.7988826815642458\n",
            "StandardScaler 0.7988826815642458\n",
            "MaxAbsScaler 0.7988826815642458\n",
            "RobustScaler 0.7988826815642458\n",
            "\n",
            "RidgeClassifierCV 0.8044692737430168\n",
            "MinMaxScaler 0.7988826815642458\n",
            "StandardScaler 0.7988826815642458\n",
            "MaxAbsScaler 0.7988826815642458\n",
            "RobustScaler 0.8044692737430168\n",
            "\n",
            "SGDClassifier 0.7821229050279329\n",
            "MinMaxScaler 0.8044692737430168\n",
            "StandardScaler 0.7486033519553073\n",
            "MaxAbsScaler 0.7821229050279329\n",
            "RobustScaler 0.7821229050279329\n",
            "\n",
            "SVC 0.7318435754189944\n",
            "MinMaxScaler 0.8491620111731844\n",
            "StandardScaler 0.8379888268156425\n",
            "MaxAbsScaler 0.8268156424581006\n",
            "RobustScaler 0.8379888268156425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier 0.8379888268156425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8435754189944135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8603351955307262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8268156424581006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8435754189944135\n",
            "\n",
            "MLPClassifier 0.8435754189944135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8324022346368715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8603351955307262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8324022346368715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8547486033519553\n",
            "XGboost : 0.8379888268156425\n",
            "XGboost : 0.8379888268156425\n",
            "XGboost : 0.8379888268156425\n",
            "XGboost : 0.8379888268156425\n",
            "XGboost : 0.8379888268156425\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PassiveAggressiveClassifier 0.7486033519553073\n",
            "MinMaxScaler 0.4134078212290503\n",
            "StandardScaler 0.6703910614525139\n",
            "MaxAbsScaler 0.7541899441340782\n",
            "RobustScaler 0.6815642458100558\n",
            "\n",
            "RidgeClassifier 0.7821229050279329\n",
            "MinMaxScaler 0.7877094972067039\n",
            "StandardScaler 0.7877094972067039\n",
            "MaxAbsScaler 0.7877094972067039\n",
            "RobustScaler 0.7821229050279329\n",
            "\n",
            "RidgeClassifierCV 0.7821229050279329\n",
            "MinMaxScaler 0.7877094972067039\n",
            "StandardScaler 0.7821229050279329\n",
            "MaxAbsScaler 0.7877094972067039\n",
            "RobustScaler 0.7821229050279329\n",
            "\n",
            "SGDClassifier 0.6312849162011173\n",
            "MinMaxScaler 0.8491620111731844\n",
            "StandardScaler 0.7932960893854749\n",
            "MaxAbsScaler 0.8100558659217877\n",
            "RobustScaler 0.8212290502793296\n",
            "\n",
            "SVC 0.7262569832402235\n",
            "MinMaxScaler 0.8491620111731844\n",
            "StandardScaler 0.8268156424581006\n",
            "MaxAbsScaler 0.8156424581005587\n",
            "RobustScaler 0.8324022346368715\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier 0.770949720670391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8379888268156425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8547486033519553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8100558659217877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8379888268156425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier 0.770949720670391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8491620111731844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8659217877094972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8268156424581006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8491620111731844\n",
            "XGboost : 0.8659217877094972\n",
            "XGboost : 0.8659217877094972\n",
            "XGboost : 0.8659217877094972\n",
            "XGboost : 0.8659217877094972\n",
            "XGboost : 0.8659217877094972\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PassiveAggressiveClassifier 0.6783216783216783\n",
            "MinMaxScaler 0.8321678321678322\n",
            "StandardScaler 0.7622377622377622\n",
            "MaxAbsScaler 0.6713286713286714\n",
            "RobustScaler 0.6713286713286714\n",
            "\n",
            "RidgeClassifier 0.8041958041958042\n",
            "MinMaxScaler 0.8111888111888111\n",
            "StandardScaler 0.8041958041958042\n",
            "MaxAbsScaler 0.8111888111888111\n",
            "RobustScaler 0.8041958041958042\n",
            "\n",
            "RidgeClassifierCV 0.8041958041958042\n",
            "MinMaxScaler 0.8111888111888111\n",
            "StandardScaler 0.8041958041958042\n",
            "MaxAbsScaler 0.8111888111888111\n",
            "RobustScaler 0.8041958041958042\n",
            "\n",
            "SGDClassifier 0.7762237762237763\n",
            "MinMaxScaler 0.7972027972027972\n",
            "StandardScaler 0.7832167832167832\n",
            "MaxAbsScaler 0.6153846153846154\n",
            "RobustScaler 0.7692307692307693\n",
            "\n",
            "SVC 0.6433566433566433\n",
            "MinMaxScaler 0.8321678321678322\n",
            "StandardScaler 0.8461538461538461\n",
            "MaxAbsScaler 0.8321678321678322\n",
            "RobustScaler 0.8391608391608392\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier 0.8321678321678322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8251748251748252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8251748251748252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8391608391608392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8251748251748252\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier 0.8391608391608392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaler 0.8391608391608392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "StandardScaler 0.8321678321678322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MaxAbsScaler 0.8391608391608392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RobustScaler 0.8251748251748252\n",
            "XGboost : 0.7902097902097902\n",
            "XGboost : 0.7902097902097902\n",
            "XGboost : 0.7902097902097902\n",
            "XGboost : 0.7902097902097902\n",
            "XGboost : 0.7902097902097902\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eL5f95CTI8Q",
        "outputId": "35163dfb-b495-4ed3-c4f4-5ff1d06d7c4d"
      },
      "source": [
        "# total dataset Train for release \n",
        "totalSet = processed_df[0]\n",
        "label = list(totalSet.columns)\n",
        "\n",
        "label.remove('Survived')\n",
        "\n",
        "\n",
        "X = totalSet[label]\n",
        "std = StandardScaler().fit(X)\n",
        "std_X = std.transform(X)\n",
        "y = totalSet['Survived']\n",
        "mlp = MLPClassifier(hidden_layer_sizes=[70], max_iter=300, random_state=411).fit(std_X, y)\n",
        "print()\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfBjxLbBYvn_",
        "outputId": "0f9ebb37-5ac3-4813-cb2e-accbb28bca76"
      },
      "source": [
        "# prediction \n",
        "# feature_label.remove(\"Survived\")\n",
        "test = test_df[feature_label]\n",
        "test = dataProcessing(test)\n",
        "test = oneHotEncoding(test)\n",
        "\n",
        "testX = test[0]\n",
        "print(testX.shape)\n",
        "std_testX = std.fit_transform(testX)\n",
        "print(std_testX.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(418, 10)\n",
            "(418, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pRBehn5naVq"
      },
      "source": [
        "predict = mlp.predict(std_testX)\n",
        "concat = predict.reshape(-1,1)\n",
        "test = test_df[\"PassengerId\"]\n",
        "result = pd.concat([test, pd.DataFrame(concat, columns=[\"Survived\"])], axis=1)\n",
        "\n",
        "\n",
        "result.to_csv(\"result.csv\", index=False)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghPEJd4yO_Vt"
      },
      "source": [
        "# import torch\n",
        "# import torchvision \n",
        "# import torch.nn as nn\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#   def __init__(self, input_size, channel, output):\n",
        "#     super(Model, self).__init__()\n",
        "\n",
        "#     self.mlp = nn.Seqential(\n",
        "        \n",
        "#         nn.lienar\n",
        "#     )\n"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGFoJJTMm7i1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}